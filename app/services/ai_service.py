"""
AI Service Layer

Este m√≥dulo fornece fun√ß√µes de alto n√≠vel para intera√ß√£o com a camada de IA.
Ele atua como uma ponte entre os endpoints da API e a orquestra√ß√£o
LangChain + Gemini definida em `ai_chain.py`.
"""

import logging
from app.services.ai_chain import (
    process_chat_message,
    process_with_langchain,
    clear_conversation_memory,
    get_conversation_summary,
    ai_orchestrator,
)

# Configure logging
logger = logging.getLogger(__name__)


# -----------------------------
# Fun√ß√µes principais de servi√ßo
# -----------------------------

async def process_chat_message_service(
    message: str, session_id: str = "default"
) -> str:
    """
    Processa a mensagem do usu√°rio usando LangChain + Gemini.
    """
    try:
        logger.info(f"üì® Processando mensagem: {message[:50]}... (sess√£o={session_id})")

        # Sempre processa via LangChain
        response = await process_with_langchain(message, session_id=session_id)

        logger.info(f"‚úÖ Resposta gerada: {response[:50]}...")
        return response

    except Exception as e:
        logger.error(f"‚ùå Erro no processamento da mensagem: {str(e)}")
        return (
            "Desculpe, ocorreu um erro ao processar sua mensagem. "
            "Por favor, tente novamente mais tarde."
        )


async def get_ai_service_status() -> dict:
    """
    Retorna o status atual do servi√ßo de IA.
    """
    try:
        # Testa rapidamente se a IA responde
        test_response = await process_with_langchain("teste", session_id="__status__")
        ai_orchestrator.clear_session_memory("__status__")

        return {
            "service": "ai_service",
            "status": "active",
            "message": "LangChain + Gemini est√° operacional",
            "test_response": test_response[:50],
            "system_prompt_configured": bool(ai_orchestrator.system_prompt),
            "active_sessions": len(ai_orchestrator.get_conversation_summary("default")),
            "features": [
                "langchain_integration",
                "gemini_api",
                "conversation_memory",
                "session_management",
                "brazilian_portuguese_responses",
            ],
        }

    except Exception as e:
        logger.error(f"‚ùå Erro ao verificar status da IA: {str(e)}")
        return {
            "service": "ai_service",
            "status": "error",
            "error": str(e),
            "configuration_required": True,
        }


# -----------------------------
# Aliases para compatibilidade
# -----------------------------

# Alias para manter compatibilidade com `chat.py`
process_chat_message = process_chat_message_service

# Processamento de mensagens (compat√≠vel com vers√µes antigas)
process_ai_message = process_chat_message_service

# Mem√≥ria da conversa
clear_memory = clear_conversation_memory
get_summary = get_conversation_summary
